{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a05f7cb-5d65-45be-8972-f2b4a1026f8e",
   "metadata": {},
   "source": [
    "# Beautiful Soup Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d11908-8451-47a3-89ce-c753ab0aa272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d417c9-cbe8-412e-bed6-cb4a266447ba",
   "metadata": {},
   "source": [
    "In this lesson, we'll use beautiful soup and openai to develop a web scraper for the indeed website, storing the data in postgres.  We'll also use the SQLAlchemy and Flask libraries to use that data for a backend API.  Let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff2b7b-746a-4322-a281-3e88192dce03",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13a77-f83a-4eca-82fc-cda206e0029f",
   "metadata": {},
   "source": [
    "Let's begin by exploring the Indeed.com website.  In doing so, what we're looking for is the url we can make a request to, that we can ultimately scrape.\n",
    "\n",
    "Ok, so go to Indeed.com, and then see how it works by typing in the job title `Data Engineer`, and a location of `New York, NY`, then click on `Find Jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2ba7d-646d-4d20-875a-91fdecd0fcaa",
   "metadata": {},
   "source": [
    "<img src=\"data-eng-jobs.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6500570-d6a6-4fcb-ab79-1bb6019941b9",
   "metadata": {},
   "source": [
    "Finally, click on the second page of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e150c8-7fdd-4101-9534-9b45a67d91a0",
   "metadata": {},
   "source": [
    "<img src=\"./second-results.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b31af-5267-422e-a50e-73d1148df710",
   "metadata": {},
   "source": [
    "The key thing to really pay attention to is the url at the top as we navigate the website.  As we can see we have a url of `indeed.com/jobs` with various parameters.  \n",
    "\n",
    "The `start=10` is a pagination parameter, which allows us to page page through results.  \n",
    "\n",
    "> So here, we are not seeing the results at the very top, but from number 10 on, as we are on the second page and there are 10 results per page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf529b-d302-4fae-b097-3a1c0cfbc866",
   "metadata": {},
   "source": [
    "<img src=\"./indeed-url.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f525c2-e54d-41c4-a88b-d027838ab78c",
   "metadata": {},
   "source": [
    "Ok, so now it's time to write our first function.  Before doing so, first create a new python environment, and activate the environment.\n",
    "\n",
    "Then install the necessary libraries for the project, which are listed in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf740f4-bbbc-452e-8fe7-e699dcddb99d",
   "metadata": {},
   "source": [
    "You can install these by running:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bae52-f2b2-411b-8b80-80d4cf87ac69",
   "metadata": {},
   "source": [
    "`pip3 install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cced5a-1c70-4449-b2e0-4d95aad6bbd5",
   "metadata": {},
   "source": [
    "Then, you can run the tests for the `indeed_client` with the command:\n",
    "\n",
    "```bash\n",
    "python3 -m pytest tests/test_indeed_client.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0264152-defb-4cfa-8d7f-d405a25cd6f7",
   "metadata": {},
   "source": [
    "### Working with the Indeed client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f0fca-df80-47dd-839f-807200d76711",
   "metadata": {},
   "source": [
    "Ok, so the first file we should work is the `indeed_client.py` file.  By client, we mean something that interacts directly with the external website -- `indeed.com`.\n",
    "\n",
    "* `get_indeed_html` - `provided`\n",
    "\n",
    "In that file, we wrote a function called `get_indeed_html` uses selenium to make a request to indeed.com.  It should automatically install the chrome driver, which you can see  more information about [here](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efa5be-6d73-496b-a0b4-aa87189a9295",
   "metadata": {},
   "source": [
    "* `get_job_cards`\n",
    "    * Now the `get_indeed_html` function returns the HTML from the entire page, so we now would like to write a function called `get_job_cards` calls our `get_indeed_html` function, and then selects the list of job cards on each page.  Notice that the relevant content appears to be located in the `td` items.\n",
    "    * Pass the related test in the `test_indeed_client.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40aab0-4e69-45c6-a9cc-38d39c91ed3d",
   "metadata": {},
   "source": [
    "* `extract_text_from_card` - `provided`\n",
    "    * Here the function, retrieves the text from a provided card.  We do some light clean up by using `strip` to remove whitespaces, and making sure we remove css.\n",
    "    \n",
    "* `get_id_from_card`\n",
    "    * Each listed position also has an indeed id.  We would like that in addition to the text from the card.  Pass the related test.\n",
    "    \n",
    "    > Hint: For `get_id_from_card` look to the `a tag` nested inside of an individual `td`, and on that a tag, you can find the `data-jk` attribute that has the id.  In the image below, the id begins with `8cba`.\n",
    "\n",
    "    > <img src=\"./data-jk-a.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3566f-ff8b-42e6-acb7-4fd606ecaeec",
   "metadata": {},
   "source": [
    "* `get_card_info`\n",
    "    * This simply calls `extract_text_from_card` to pull out a list of the text elements, and then appends one additional string of \n",
    "    * `\"job id: job_id\"`\n",
    "    * where `job_id` is the job_id we retrieved from the earlier method. \n",
    "    \n",
    "* `get_card_infos`\n",
    "    * This retrieves the card info (including the job_id) for each card in the html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc6e7f-7337-4e76-9d37-4078be0d03c0",
   "metadata": {},
   "source": [
    "### Writing to a file\n",
    "\n",
    "Ok, now so far we have retrieved some text.  Notice that this text is not perfectly clean.  But we don't need it to be perfectly clean -- we'll let openai interpret this text.  But first, we'llÂ build some methods to write our text to a file.  From there, we'll have openai read this text.\n",
    "\n",
    "Ok, so now for the methods on writing our text to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede58cdf-f49b-4908-a38f-72cf7ec50efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "* `retrieve_text`\n",
    "    * The first step is a method that will turn our `card_infos` list into some text.  So for each card_info list, add a `\\n` to separate each element in a card info by a new line.  Then separate each card_info by two lines.  See the related test.\n",
    "   * This is an example of how the card info texts should be formatted:\n",
    "```\n",
    "Data Engineer\n",
    "HealthFirst\n",
    "Staten Island, NY 10301\n",
    "(\n",
    "New Brighton area\n",
    ")\n",
    "Pay information not provided\n",
    "Full-time\n",
    "job id: af1c846c34ac0534\n",
    "\n",
    "Data Engineer\n",
    "NYPD Civilian Jobs\n",
    "Manhattan, NY\n",
    "job id: 9562c51b70acd54d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70743c-1eea-452d-82e9-ec8631c2ceee",
   "metadata": {},
   "source": [
    "* `directory_name_builder`\n",
    "\n",
    "Ok, so we are about to write this text to a file, but before we do we should build the `directory_name_builder` function.  This function generates the directory name that we will write the file to.   It takes 4 arguments, the `position`, `location`, `directory` and `date`.  So if there are arguments `directory_name_builder('data engineer', 'united states')` and the folder is built in the format of something like:\n",
    "\n",
    "`..data/text_docs/data_engineer/nyc/2024-03-01`\n",
    "\n",
    "So there is a directory should have a default argument of `..data/text_docs` and date has a default argument of `today`.  When that argument is today, the current date should be the inner most folder.  Notice that everything is lower case and there are no spaces in the folder name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef996667-85df-430c-b2e6-a7ef0972b46e",
   "metadata": {
    "tags": []
   },
   "source": [
    "* `write_to_file` - `provided`\n",
    "    * You can see that `write_to_file` uses the folder name generated from `directory_name_builder` and then creates this directory if it does not exist, and then writes the provided text to a file whose last character is the job index being scraped (remember that we can paginate through the jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b1d5a-eb28-4641-8e74-31b32dcdc37f",
   "metadata": {},
   "source": [
    "* `retrieve_and_write_pages` - `provided`\n",
    "    * This will loop through a specified number of pages.  It has a step size because we want to pull 15 positions at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f05c89-5364-4d8a-83db-ed276502500f",
   "metadata": {},
   "source": [
    "Ok, so if you call `retrieve_and_write_pages` from the `main.py` or from the console, then you should see the relevant folder generated, and files generated with text of various positions inside of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f0b40-f521-4722-8d5a-57ce9bcbb32c",
   "metadata": {},
   "source": [
    "## OpenAI Text to Json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ace545-c0b9-40da-9191-9a10d929933f",
   "metadata": {},
   "source": [
    "Next, we can use openai's API to generate json from the data.  You can learn more about how to do that from [this resource](https://community.openai.com/t/how-do-i-use-the-new-json-mode/475890/11).  \n",
    "\n",
    "This step is more prompt engineering than anything, and so we provided the code of our two methods for you.  \n",
    "\n",
    "* `build_prompt`\n",
    "\n",
    "This generates the prompt that we will provide to openai.  You can see the prompt here.\n",
    "    * `prompt = f\"\"\"Format in json, the job_title, company_name, min_salary, max_salary, location, and presence as in-person, remote, hybrid or unknown of each of the jobs in the context.\n",
    "\n",
    "    The json schema should include: \n",
    "\n",
    "    {JSON_SCHEMAS}\n",
    "\n",
    "    Example:\n",
    "\n",
    "\n",
    "    Senior Data Engineer\n",
    "    Disney Entertainment & ESPN Technology\n",
    "    New York, NY\n",
    "    $136,038 - $182,490 a year\n",
    "    job id: afece6001fb4eb54\n",
    "\n",
    "    {ex_1}\n",
    "\n",
    "\n",
    "    Context:\n",
    "\n",
    "    {file_text}\n",
    "    \"\"\"   \n",
    "`\n",
    "\n",
    "So we tell openai what to do, and then we provide it a JSON schema of the output format it should generate.  You can see that in that schema we provide the key and for the value the datatype and a small description.\n",
    "```python\n",
    "JSON_SCHEMAS =  {\n",
    "        \"job_id\": \"string\",\n",
    "        \"job_title\": \"string (do not include information about senority level, Good: data engineer, Bad: Senior data engineer)\",\n",
    "```\n",
    "\n",
    "* One shot learning \n",
    "\n",
    "We then provide an example of an input and the output it should generate. \n",
    "\n",
    "```text\n",
    "Example\n",
    "\n",
    "Senior Data Engineer\n",
    "Disney Entertainment & ESPN Technology\n",
    "New York, NY\n",
    "$136,038 - $182,490 a year\n",
    "job id: afece6001fb4eb54\n",
    "\n",
    "ex_1 = {'job_id': 'afece6001fb4eb54', 'job_title': 'Data Engineer',\n",
    "    'company_name': 'Disney Entertainment & ESPN Technology',\n",
    "    'seniority_level': 'senior', 'min_salary': 136038,\n",
    "    'max_salary': 182490, 'city': 'New York',\n",
    "        'state': 'NY', 'zipcode': None, 'presence': 'unknown'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954abc01-8441-4bce-aa2b-50d9cfc2dc17",
   "metadata": {},
   "source": [
    "So this can help the AI model learn what it should produce.  If we provide no example it's called **zero shot learning**, one example is **one-shot learning**, and then two examples is **two-shot learning**.  No one has ever tried three.\n",
    "\n",
    "Then after this example, we provide it the text from the file (the list of positions in a similar format to our example), and hopefully it will generate the json.\n",
    "\n",
    "* return_json_from(prompt)\n",
    "\n",
    "Ok, so this takes the text from the prompt, and feeds it to the model.  For the output to be json data, only specific openai models can be used, and we use `\"gpt-3.5-turbo-1106\"`.  Notice our response format: `response_format={ \"type\": \"json_object\" }`.\n",
    "\n",
    "Finally it outputs a string, so we use `json_response = json.loads(json_content)` to turn that string into a list of dictionaries one for each position in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399d7e2-2a85-4ba3-83ea-4ea94d27e7b7",
   "metadata": {},
   "source": [
    "# FileReader \n",
    "\n",
    "Ok, so now remember what we have built so far.\n",
    "\n",
    "1. We now have code that scrapes positions from the indeed website, and writes them to the specified file.\n",
    "2. We have code that can take text in the format of our file, and use openai to return json.\n",
    "\n",
    "\n",
    "So the next step is to read the text from that file and from there we can use our `json_builder` to properly format it into json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09e54d-cd43-4881-8606-aa555e41af07",
   "metadata": {},
   "source": [
    "* `file_to_df(file_name)`\n",
    "\n",
    "Given a file name, it should return the jobs in the file as a dataframe.  However, if the file has fewer than 20 characters, it should just return an empty dataframe.\n",
    "\n",
    "* To build this, use the functions in our `json_builder` file.\n",
    "* Before returning the dataframe, replace any values of `nan` or `unknown` in the dataframe with `None` -- this way when we ultimately persist this data in a database, it will be saved as null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85740cf-772c-46c0-a428-e1f417309c24",
   "metadata": {},
   "source": [
    "* `parse_from_file_name`\n",
    "    * Now we're close to loading this data into a database.  However, beyond just loading each position into the database, we'll also want to associate that position with the scraping -- that is the html and date pulling the data.  Luckily, that information is encoded in the folder structure.\n",
    "    So write a function called `parse_from_file_name` that a file like: \n",
    "        `text_docs/data_engineer/united_states/2024-03-01/results_2.txt`, will return a dictionary of each of the attributes (`position`, `location`, `date` and `job_idx`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597230e-473a-4043-aeef-cd8c7eb6674a",
   "metadata": {},
   "source": [
    "### Developing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55307d3b-0298-49bf-98d8-7940b29a375e",
   "metadata": {},
   "source": [
    "So now we have written code to scrape our html, save the text, and then extract data both from the text file, and from the *name* of the text file.  Next up is to save this data to a database.\n",
    "\n",
    "To do this we'll need to code for two models: scraping and position, where a scraping has many positions.  We'll use flask_sqlalchemy to do this.  This requires a bit of setup.\n",
    "\n",
    "1. `.flaskenv` - specify that we'll be using `server.py` as the location of the `FLASK_APP`.\n",
    "2. `app/__init__.py` Here, we'll write the `create_app` function.  This should take the `db_conn` string as an argument, which can be imported from settings, which pulls the data from the .env file.  The name of my connection string is: \n",
    "    * `postgresql://localhost:5432/indeed_llm_scraper`\n",
    "3. `server.py`\n",
    "    * This is where we'll ultimately create the app, passing through the `db_conn` string, and setting up sqlalchemy with it.\n",
    "\n",
    "Ok, once the setup is complete, the models can be written.\n",
    "\n",
    "1. Position\n",
    "2. Scraping \n",
    "\n",
    "You can see the underlying columns for the positions and scrapings models in the `migrations/create_tables.sql` folder.  Don't forget to also add the relationships so that a scraping has many positions.  Get the corresponding tests in the `tests/models` folder to pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ef54-189d-4cee-866b-359f18825fa5",
   "metadata": {},
   "source": [
    "TODO - Finish tests for the sql relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b78e8-cb10-470d-9d52-5079d55f1829",
   "metadata": {},
   "source": [
    "### Back to FileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44e5c0-d003-4357-a386-90f490b8bb88",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8720c4c-6d12-4c12-9b00-51cc765349bf",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff080302-d004-4f57-b8c6-655e14164259",
   "metadata": {},
   "source": [
    "In this lesson, we used our knowledge of requests, beautiful soup and objects to build an indeed scraper.  The pattern that we used is called the adapter pattern.  With that pattern, we used a *client* to interact directly with the web site, and then passed the retrieved information to the adapter which extracted the related information and created a position instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
