{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a05f7cb-5d65-45be-8972-f2b4a1026f8e",
   "metadata": {},
   "source": [
    "# Beautiful Soup Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d11908-8451-47a3-89ce-c753ab0aa272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d417c9-cbe8-412e-bed6-cb4a266447ba",
   "metadata": {},
   "source": [
    "In this lesson, we'll use beautiful soup and openai to develop a web scraper for the indeed website, storing the data in postgres.  We'll also use the SQLAlchemy and Flask libraries to use that data for a backend API.  Let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff2b7b-746a-4322-a281-3e88192dce03",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13a77-f83a-4eca-82fc-cda206e0029f",
   "metadata": {},
   "source": [
    "Let's begin by exploring the Indeed.com website.  In doing so, what we're looking for is the url we can make a request to, that we can ultimately scrape.\n",
    "\n",
    "Ok, so go to Indeed.com, and then see how it works by typing in the job title `Data Engineer`, and a location of `New York, NY`, then click on `Find Jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2ba7d-646d-4d20-875a-91fdecd0fcaa",
   "metadata": {},
   "source": [
    "<img src=\"data-eng-jobs.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6500570-d6a6-4fcb-ab79-1bb6019941b9",
   "metadata": {},
   "source": [
    "Finally, click on the second page of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e150c8-7fdd-4101-9534-9b45a67d91a0",
   "metadata": {},
   "source": [
    "<img src=\"./second-results.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b31af-5267-422e-a50e-73d1148df710",
   "metadata": {},
   "source": [
    "The key thing to really pay attention to is the url at the top as we navigate the website.  As we can see we have a url of `indeed.com/jobs` with various parameters.  \n",
    "\n",
    "The `start=10` is a pagination parameter, which allows us to page page through results.  \n",
    "\n",
    "> So here, we are not seeing the results at the very top, but from number 10 on, as we are on the second page and there are 10 results per page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf529b-d302-4fae-b097-3a1c0cfbc866",
   "metadata": {},
   "source": [
    "<img src=\"./indeed-url.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f525c2-e54d-41c4-a88b-d027838ab78c",
   "metadata": {},
   "source": [
    "Ok, so now it's time to write our first function.  Before doing so, first create a new python environment, and activate the environment.\n",
    "\n",
    "Then install the necessary libraries for the project, which are listed in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf740f4-bbbc-452e-8fe7-e699dcddb99d",
   "metadata": {},
   "source": [
    "You can install these by running:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bae52-f2b2-411b-8b80-80d4cf87ac69",
   "metadata": {},
   "source": [
    "`pip3 install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cced5a-1c70-4449-b2e0-4d95aad6bbd5",
   "metadata": {},
   "source": [
    "Then, you can run the tests for the `indeed_client` with the command:\n",
    "\n",
    "```bash\n",
    "python3 -m pytest tests/test_indeed_client.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0264152-defb-4cfa-8d7f-d405a25cd6f7",
   "metadata": {},
   "source": [
    "### Working with the Indeed client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f0fca-df80-47dd-839f-807200d76711",
   "metadata": {},
   "source": [
    "Ok, so the first file we should work is the `indeed_client.py` file.  By client, we mean something that interacts directly with the external website -- `indeed.com`.\n",
    "\n",
    "* `get_indeed_html` - `provided`\n",
    "\n",
    "In that file, we wrote a function called `get_indeed_html` uses selenium to make a request to indeed.com.  It should automatically install the chrome driver, which you can see  more information about [here](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efa5be-6d73-496b-a0b4-aa87189a9295",
   "metadata": {},
   "source": [
    "* `get_job_cards`\n",
    "    * Now the `get_indeed_html` function returns the HTML from the entire page, so we now would like to write a function called `get_job_cards` calls our `get_indeed_html` function, and then selects the list of job cards on each page.  Notice that the relevant content appears to be located in the `td` items.\n",
    "    * Pass the related test in the `test_indeed_client.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40aab0-4e69-45c6-a9cc-38d39c91ed3d",
   "metadata": {},
   "source": [
    "* `extract_text_from_card` - `provided`\n",
    "    * Here the function, retrieves the text from a provided card.  We do some light clean up by using `strip` to remove whitespaces, and making sure we remove css.\n",
    "    \n",
    "* `get_id_from_card`\n",
    "    * Each listed position also has an indeed id.  We would like that in addition to the text from the card.  Pass the related test.\n",
    "    \n",
    "    > Hint: For `get_id_from_card` look to the `a tag` nested inside of an individual `td`, and on that a tag, you can find the `data-jk` attribute that has the id.  In the image below, the id begins with `8cba`.\n",
    "\n",
    "    > <img src=\"./data-jk-a.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3566f-ff8b-42e6-acb7-4fd606ecaeec",
   "metadata": {},
   "source": [
    "* `get_card_info`\n",
    "    * This simply calls `extract_text_from_card` to pull out a list of the text elements, and then appends one additional string of \n",
    "    * `\"job id: job_id\"`\n",
    "    * where `job_id` is the job_id we retrieved from the earlier method. \n",
    "    \n",
    "* `get_card_infos`\n",
    "    * This retrieves the card info (including the job_id) for each card in the html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc6e7f-7337-4e76-9d37-4078be0d03c0",
   "metadata": {},
   "source": [
    "### Writing to a file\n",
    "\n",
    "Ok, now so far we have retrieved some text.  Notice that this text is not perfectly clean.  But we don't need it to be perfectly clean -- we'll let openai interpret this text.  But first, we'llÂ build some methods to write our text to a file.  From there, we'll have openai read this text.\n",
    "\n",
    "Ok, so now for the methods on writing our text to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede58cdf-f49b-4908-a38f-72cf7ec50efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "* `build_text`\n",
    "    * The first step is a method that will turn our `card_infos` list into some text.  So for each card_info list, add a `\\n` to separate each element in a card info by a new line.  Then separate each card_info by two lines.  See the related test.\n",
    "   * This is an example of how the card info texts should be formatted:\n",
    "```\n",
    "Data Engineer\n",
    "HealthFirst\n",
    "Staten Island, NY 10301\n",
    "(\n",
    "New Brighton area\n",
    ")\n",
    "Pay information not provided\n",
    "Full-time\n",
    "job id: af1c846c34ac0534\n",
    "\n",
    "Data Engineer\n",
    "NYPD Civilian Jobs\n",
    "Manhattan, NY\n",
    "job id: 9562c51b70acd54d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70743c-1eea-452d-82e9-ec8631c2ceee",
   "metadata": {},
   "source": [
    "* `directory_name_builder`\n",
    "\n",
    "Ok, so we are about to write this text to a file, but before we do we should build the `directory_name_builder` function.  This function generates the directory name that we will write the file to.   It takes 4 arguments, the `position`, `location`, `directory` and `date`.  So if there are arguments `directory_name_builder('data engineer', 'united states')` and the folder is built in the format of something like:\n",
    "\n",
    "`..data/text_docs/data_engineer/nyc/2024-03-01`\n",
    "\n",
    "So there is a directory should have a default argument of `..data/text_docs` and date has a default argument of `today`.  When that argument is today, the current date should be the inner most folder.  Notice that everything is lower case and there are no spaces in the folder name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef996667-85df-430c-b2e6-a7ef0972b46e",
   "metadata": {
    "tags": []
   },
   "source": [
    "* `write_to_file` - `provided`\n",
    "    * You can see that `write_to_file` uses the folder name generated from `directory_name_builder` and then creates this directory if it does not exist, and then writes the provided text to a file whose last character is the job index being scraped (remember that we can paginate through the jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b1d5a-eb28-4641-8e74-31b32dcdc37f",
   "metadata": {},
   "source": [
    "* `retrieve_and_write_pages` - `provided`\n",
    "    * This will loop through a specified number of pages.  It has a step size because we want to pull 15 positions at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f05c89-5364-4d8a-83db-ed276502500f",
   "metadata": {},
   "source": [
    "Ok, so if you call `retrieve_and_write_pages` from the `main.py` or from the console, then you should see the relevant folder generated, and files generated with text of various positions inside of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f0b40-f521-4722-8d5a-57ce9bcbb32c",
   "metadata": {},
   "source": [
    "## OpenAI Text to Json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ace545-c0b9-40da-9191-9a10d929933f",
   "metadata": {},
   "source": [
    "Next, we can use openai's API to generate json from the data.  You can learn more about how to do that from [this resource](https://community.openai.com/t/how-do-i-use-the-new-json-mode/475890/11).  \n",
    "\n",
    "This step is more prompt engineering than anything, and so we provided the code of our two methods for you.  \n",
    "\n",
    "* `build_prompt`\n",
    "\n",
    "This generates the prompt that we will provide to openai.  You can see the prompt here.\n",
    "    * `prompt = f\"\"\"Format in json, the job_title, company_name, min_salary, max_salary, location, and presence as in-person, remote, hybrid or unknown of each of the jobs in the context.\n",
    "\n",
    "    The json schema should include: \n",
    "\n",
    "    {JSON_SCHEMAS}\n",
    "\n",
    "    Example:\n",
    "\n",
    "\n",
    "    Senior Data Engineer\n",
    "    Disney Entertainment & ESPN Technology\n",
    "    New York, NY\n",
    "    $136,038 - $182,490 a year\n",
    "    job id: afece6001fb4eb54\n",
    "\n",
    "    {ex_1}\n",
    "\n",
    "\n",
    "    Context:\n",
    "\n",
    "    {file_text}\n",
    "    \"\"\"   \n",
    "`\n",
    "\n",
    "So we tell openai what to do, and then we provide it a JSON schema of the output format it should generate.  You can see that in that schema we provide the key and for the value the datatype and a small description.\n",
    "```python\n",
    "JSON_SCHEMAS =  {\n",
    "        \"job_id\": \"string\",\n",
    "        \"job_title\": \"string (do not include information about senority level, Good: data engineer, Bad: Senior data engineer)\",\n",
    "```\n",
    "\n",
    "* One shot learning \n",
    "\n",
    "We then provide an example of an input and the output it should generate. \n",
    "\n",
    "```text\n",
    "Example\n",
    "\n",
    "Senior Data Engineer\n",
    "Disney Entertainment & ESPN Technology\n",
    "New York, NY\n",
    "$136,038 - $182,490 a year\n",
    "job id: afece6001fb4eb54\n",
    "\n",
    "ex_1 = {'job_id': 'afece6001fb4eb54', 'job_title': 'Data Engineer',\n",
    "    'company_name': 'Disney Entertainment & ESPN Technology',\n",
    "    'seniority_level': 'senior', 'min_salary': 136038,\n",
    "    'max_salary': 182490, 'city': 'New York',\n",
    "        'state': 'NY', 'zipcode': None, 'presence': 'unknown'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954abc01-8441-4bce-aa2b-50d9cfc2dc17",
   "metadata": {},
   "source": [
    "So this can help the AI model learn what it should produce.  If we provide no example it's called **zero shot learning**, one example is **one-shot learning**, and then two examples is **two-shot learning**.  No one has ever tried three.\n",
    "\n",
    "Then after this example, we provide it the text from the file (the list of positions in a similar format to our example), and hopefully it will generate the json.\n",
    "\n",
    "* `return_json_from(prompt)`\n",
    "\n",
    "Ok, so this takes the text from the prompt, and feeds it to the model.  For the output to be json data, only specific openai models can be used, and we use `\"gpt-3.5-turbo-1106\"`.  Notice our response format: `response_format={ \"type\": \"json_object\" }`.\n",
    "\n",
    "Finally it outputs a string, so we use `json_response = json.loads(json_content)` to turn that string into a list of dictionaries one for each position in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399d7e2-2a85-4ba3-83ea-4ea94d27e7b7",
   "metadata": {},
   "source": [
    "# FileReader \n",
    "\n",
    "Ok, so now remember what we have built so far.\n",
    "\n",
    "1. We now have code that scrapes positions from the indeed website, and writes them to the specified file.\n",
    "2. We have code that can take text in the format of our file, and use openai to return json.\n",
    "\n",
    "\n",
    "So the next step is to read the text from that file and from there we can use our `json_builder` to properly format it into json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09e54d-cd43-4881-8606-aa555e41af07",
   "metadata": {},
   "source": [
    "* `file_to_df(file_name)`\n",
    "\n",
    "Given a file name, it should return the jobs in the file as a dataframe.  However, if the file has fewer than 20 characters, it should just return an empty dataframe.\n",
    "\n",
    "* To build this, use the functions in our `json_builder` file.\n",
    "* Before returning the dataframe, replace any values of `nan`, `''` or `unknown` in the dataframe with `None` -- this way when we ultimately persist this data in a database, it will be saved as null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85740cf-772c-46c0-a428-e1f417309c24",
   "metadata": {},
   "source": [
    "* `parse_from_file_name`\n",
    "    * Now we're close to loading this data into a database.  However, beyond just loading each position into the database, we'll also want to associate that position with the scraping -- that is the html and date pulling the data.  Luckily, that information is encoded in the folder structure.\n",
    "    So write a function called `parse_from_file_name` that a file like: \n",
    "        `text_docs/data_engineer/united_states/2024-03-01/results_2.txt`, will return a dictionary of each of the attributes (`position`, `location`, `date` and `job_idx`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597230e-473a-4043-aeef-cd8c7eb6674a",
   "metadata": {},
   "source": [
    "### Developing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55307d3b-0298-49bf-98d8-7940b29a375e",
   "metadata": {},
   "source": [
    "So now we have written code to scrape our html, save the text, and then extract data both from the text file, and from the *name* of the text file.  Next up is to save this data to a database.\n",
    "\n",
    "To do this we'll need to code for two models: scraping and position, where a scraping has many positions.  We'll use flask_sqlalchemy to do this.  This requires a bit of setup.  \n",
    "\n",
    "> **Hint** You can get a sense of how we set this up by referencing [this repository](https://github.com/apis-jigsaw/sql-alchemy-relations/tree/main/src) and [this lesson](https://colab.research.google.com/github/apis-jigsaw/sql-alchemy-relations/blob/main/lesson/index.ipynb).\n",
    "\n",
    "0. Create the database, and specify the name of the environment in the `.env` file.  In the settings file, set that db connection string to a variable.  The name of my connection string is: `postgresql://localhost:5432/indeed_llm_scraper`.  You also will need to set up a test database, run the migrations against the development and test databases, and store the connection to the test database as an environmental variable.  You can look at the `settings.py` file to see how we use these variables.\n",
    "\n",
    "1. `.flaskenv` - specify that we'll be using `server.py` as the location of the `FLASK_APP`.\n",
    "2. `app/__init__.py` Here, we'll write the `create_app` function.  This should take the `db_conn` string as an argument.  \n",
    "3. `server.py`\n",
    "    * This is where we'll ultimately create the app and passing through the `db_conn` string.\n",
    "\n",
    "Ok, once the setup is complete, the models can be written.\n",
    "\n",
    "1. Position\n",
    "2. Scraping \n",
    "\n",
    "You can see the underlying columns for the positions and scrapings models in the `migrations/create_tables.sql` folder.  Don't forget to also add the relationships so that a scraping has many positions.  Get the corresponding tests in the `tests/models` folder to pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b78e8-cb10-470d-9d52-5079d55f1829",
   "metadata": {},
   "source": [
    "### Back to FileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346a66f-0727-4052-b43a-13eecc7b5732",
   "metadata": {},
   "source": [
    "Ok, so now that we have our models set up, we can move beyond just reading our text file and returning a dataframe, to actually saving the data in our file to a database.\n",
    "\n",
    "* `file_to_db(file_name, db_conn)`\n",
    "\n",
    "This takes both a `file_name`, and our database connection string.  Given the `file_name`, it creates (and saves) a scraping object, and it also creates each of the positions in the file, and relates them to the scraping.  \n",
    "\n",
    "Notice that to make sure that we have our sqlalchemy connection getting set up from our app before creating or saving any model instances, we'll need to write this code inside of an `app_context()`.  We provided some of this initial code for you.\n",
    "\n",
    "Pass the related test.\n",
    "\n",
    "**Then**, to pass the next test, you need to make sure that we do not duplicate our positions.  For this use the `job_id` to make sure when we call the `file_to_db` method we first check to make sure a position is not already in the database before inserting it.  Finally, have the return value be a list of positions that were added to the database (and only those positions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56629e86-8e03-4a04-ac97-cc3559488bc2",
   "metadata": {},
   "source": [
    "* `list_files` - `provided`\n",
    "\n",
    "* `files_to_db(dir_name)`\n",
    "\n",
    "Next, now that we are able to load one file into the database, it's time to set up our code so we can a load all files in a folder to a database.  Run the `dir_to_db` method and pass the related test.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bef6a3-1bab-48e2-8b98-a73b8ca0786c",
   "metadata": {},
   "source": [
    "* Testing it out \n",
    "\n",
    "Now, if you run the `main.py` file, you can see the pipeline that we built in action.  \n",
    "The `retrieve_and_write_pages` function will use our scraper to pull data from Indeed and write the text from each scraped position to a file.  It returns a list of file_names where the jobs are written to.  Then, the `files_to_db` function will take those files, turn the text to JSON (using openai) and then turn the json to scraping and position instances which are written to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe28a60-bc27-4f84-b2b0-feaa60a03852",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156b6e-e265-43b1-90f3-91dc39db83b3",
   "metadata": {},
   "source": [
    "* Building out an API\n",
    "\n",
    "Next, in the `create_app` file, we should also add an endpoint to our flask application called `positions` so that when we make a request to `/positions`, we should see the list of positions.  One issue is that the sqlalchemy instance returns extra attributes, in addition to our column attributes.  So add a `to_dict` method in the `Position` class.\n",
    "\n",
    "```python\n",
    "def to_dict(self):\n",
    "    return dict((col, getattr(self, col)) for col in self.__table__.columns.keys())\n",
    "```\n",
    "\n",
    "Use this method when returning json of each of the positions in the `/positions` route."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c5272-e8d7-4c2e-84c2-2f5a39851a74",
   "metadata": {},
   "source": [
    "We should get back something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a322f-3444-4ff8-8ce1-20176f334803",
   "metadata": {},
   "source": [
    "> <img src=\"./returned-json.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa18b29-74d1-4c3b-bf28-eecae3bfb2c5",
   "metadata": {},
   "source": [
    "### Scheduling with Prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a554f6-5979-4ee7-abfa-4e19b5b98161",
   "metadata": {},
   "source": [
    "Ok, so now that we have the code to pull data from indeed and write it to a database, the next step is to connect this to [this lesson](https://github.com/data-engineering-jigsaw/prefect-deployments/blob/main/5-prefect-deployments/code/index.py) or some of the [related documentation](https://docs.prefect.io/latest/concepts/schedules/).\n",
    "\n",
    "We have provided a `pipeline.py` file for you.\n",
    "\n",
    "Begin by creating two tasks:\n",
    "\n",
    "* `retrieve_and_write_pages(position, location, pages)`\n",
    "    * This calls the retrieve and write pages function in our indeed scraper, and should return the list of file names.\n",
    "    \n",
    "* `read_files_to_db(file_names)`\n",
    "    * This reads the files to a database. \n",
    "\n",
    "\n",
    "From there create the following flow:\n",
    "\n",
    "* `scrape_and_load_positions(positions, locations, pages)`\n",
    "    * This takes a list of positions, locations and the number of pages to scrape from each search.\n",
    "    * It should scrape and save each combination of positions and locations.\n",
    "    \n",
    "Adding a deployment to run it on a schedule.\n",
    "\n",
    "Finally, we would want something like this to be run daily.  So let's test it out by adding a deployment that scrapes data engineer, analytics engineer positions for the cities of new york city and san francisco every 100 seconds.\n",
    "\n",
    "Remember that you should be able to view the logs of this in `prefect.io`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12160ee7-7983-4722-84ac-2edb3ddb5bc2",
   "metadata": {},
   "source": [
    "### Connecting to our dashboard\n",
    "\n",
    "We already built a little dashboard of the scraped positions for you.  You can see the related code in the `frontend` folder.  To connect this to your backend do the following:\n",
    "    \n",
    "1. Call `flask run` to boot up your api\n",
    "\n",
    "2. In a new tab, navigate to the `frontend` folder, and then run `streamlit run index.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8720c4c-6d12-4c12-9b00-51cc765349bf",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68634997-f57a-46f1-9176-150ee34b7451",
   "metadata": {},
   "source": [
    "In this lesson we used BeautifulSoup to parse down some initial text, OpenAI to process that text into JSON and SQLAlchemy to save and retrieve that data in a database.  From there, connected a flask API to our database, and developed a frontend dashboard that makes requests to the API.\n",
    "\n",
    "We also set up a prefect pipeline that regularly pulls our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
