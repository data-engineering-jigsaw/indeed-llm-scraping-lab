{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a05f7cb-5d65-45be-8972-f2b4a1026f8e",
   "metadata": {},
   "source": [
    "# Beautiful Soup Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d11908-8451-47a3-89ce-c753ab0aa272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d417c9-cbe8-412e-bed6-cb4a266447ba",
   "metadata": {},
   "source": [
    "In this lesson, we'll write a scraper to look through data engineering jobs on Indeed.com.  In doing so, we'll use the selenium library combined with the beautiful soup library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff2b7b-746a-4322-a281-3e88192dce03",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13a77-f83a-4eca-82fc-cda206e0029f",
   "metadata": {},
   "source": [
    "Let's begin by exploring the Indeed.com website.  In doing so, what we're looking for is the url we can make a request to, that we can ultimately scrape.\n",
    "\n",
    "Ok, so go to Indeed.com, and then see how it works by typing in the job title `Data Engineer`, and a location of `New York, NY`, then click on `Find Jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2ba7d-646d-4d20-875a-91fdecd0fcaa",
   "metadata": {},
   "source": [
    "<img src=\"data-eng-jobs.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6500570-d6a6-4fcb-ab79-1bb6019941b9",
   "metadata": {},
   "source": [
    "Finally, click on the second page of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e150c8-7fdd-4101-9534-9b45a67d91a0",
   "metadata": {},
   "source": [
    "<img src=\"./second-results.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b31af-5267-422e-a50e-73d1148df710",
   "metadata": {},
   "source": [
    "The key thing to really pay attention to is the url at the top as we navigate the website.  As we can see we have a url of `indeed.com/jobs` with various parameters.  \n",
    "\n",
    "The `start=10` is a pagination parameter, which allows us to page page through results.  \n",
    "\n",
    "> So here, we are not seeing the results at the very top, but from number 10 on, as we are on the second page and there are 10 results per page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf529b-d302-4fae-b097-3a1c0cfbc866",
   "metadata": {},
   "source": [
    "<img src=\"./indeed-url.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f525c2-e54d-41c4-a88b-d027838ab78c",
   "metadata": {},
   "source": [
    "Ok, so now it's time to write our first function.  Before doing so, first create a new python environment, and activate the environment.\n",
    "\n",
    "Then install the necessary libraries for the project, which are listed in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf740f4-bbbc-452e-8fe7-e699dcddb99d",
   "metadata": {},
   "source": [
    "You can install these by running:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bae52-f2b2-411b-8b80-80d4cf87ac69",
   "metadata": {},
   "source": [
    "`pip3 install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cced5a-1c70-4449-b2e0-4d95aad6bbd5",
   "metadata": {},
   "source": [
    "Then, you can run the tests for the `indeed_client` with the command:\n",
    "\n",
    "```bash\n",
    "python3 -m pytest tests/test_indeed_client.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0264152-defb-4cfa-8d7f-d405a25cd6f7",
   "metadata": {},
   "source": [
    "### Working with the Indeed client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f0fca-df80-47dd-839f-807200d76711",
   "metadata": {},
   "source": [
    "Ok, so the first file we should work is the `indeed_client.py` file.  By client, we mean something that interacts directly with the external website -- `indeed.com`.\n",
    "\n",
    "In that file, we wrote a function called `get_indeed_html` uses selenium to make a request to indeed.com.  It should automatically install the chrome driver, which you can see  more information about [here](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efa5be-6d73-496b-a0b4-aa87189a9295",
   "metadata": {},
   "source": [
    "* Parsing jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be4f94-b31f-4633-a746-b6f6d6042b70",
   "metadata": {},
   "source": [
    "Now the function above returns the HTML from the entire page, so we now would like to write a function called `get_job_cards` calls our `get_indeed_html` function, and then selects the list of 25 job cards on each page.  Notice that the relevant content appears to be located in the `td` items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67640b1-f47a-46c7-9b3e-dbfc405d7d66",
   "metadata": {},
   "source": [
    "> We started the function for you, but complete it so that it returns a list of the tds.  Pass the related test in the `test_indeed_client.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40aab0-4e69-45c6-a9cc-38d39c91ed3d",
   "metadata": {},
   "source": [
    "<img src=\"./result-content.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597230e-473a-4043-aeef-cd8c7eb6674a",
   "metadata": {},
   "source": [
    "### Indeed Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44e5c0-d003-4357-a386-90f490b8bb88",
   "metadata": {},
   "source": [
    "Ok, so now that we saw how we retrieve the list of cards with the client, the next step is to move onto the adapter file.  Remember that we saw that our client is what makes requests directly to the webpage.  \n",
    "\n",
    "Well the adapter then takes that information and extracts the related information from it.  For our project, the adapter will take in our beautiful soup html object for a single data engineering position, and retrieve information about that position.\n",
    "\n",
    "If you look at the `test_indeed_adapter.py` file, you can see that we pass into the html for a single card, create a beautiful soup object out of it, and then ask our adapter to extract the related information.\n",
    "\n",
    "While the adapter extracts information for a single card, you can see that in the `index.py` file, we have a `run` function which loops through all of the cards using the adapter to extract information from each one.\n",
    "\n",
    "Ok, let's get started.\n",
    "\n",
    "In the `indeed_adapter.py` file, write code for the following functions:\n",
    "\n",
    "* `get_id` returns the related id for a position\n",
    "* `get_company_name` returns the company name of a position\n",
    "* `get_title` returns the job title\n",
    "* `get_salary_text` returns just the related salary text from a card (see the related test)\n",
    "* `clean_salary` converts each listed salary in the salary_text to a float.\n",
    "* `get_salaries` returns a list of both the minimum and maximum salary listed for a position\n",
    "* `get_city_state` returns a tuple of the city and state for a position\n",
    "\n",
    "> Hint: For `get_id` look to the `a tag` nested inside of an individual `td`, and on that a tag, you can find the `data-jk` attribute that has the id.\n",
    "\n",
    "<img src=\"./data-jk-a.png\" width=\"70%\">\n",
    "\n",
    "### Creating Models\n",
    "\n",
    "Next create a class called position, located in the `models/position.py` file.  The class should create an instance of a position which can be initialized with `id`, `title`, `salaries`, and `city`, `state`, and `company_name` of the related position.  But it should return an instance that has attributes of min_salary and max_salary instead of a single salaries attribute (see the related test in test_position.py).  \n",
    "\n",
    "Finally we'll want to write a function called `run`, located in the IndeedAdapter.  The `run` function in the IndeedAdapter should call the previously written adapter functions to retrieve all of the information related to a position, and then create an instance of the Position class, to create a position.\n",
    "\n",
    "You can see that our `save_to_json` function first calls `run` to get the list of positions and then converts each position to a dictionary to eventually write as json.  If everything works, we should be able to call the save_to_json function and see the resulting json in our data folder.  We should see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e756dc-9ff7-41ba-a38b-9f9864adc6dd",
   "metadata": {},
   "source": [
    "<img src=\"./json-data.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8720c4c-6d12-4c12-9b00-51cc765349bf",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff080302-d004-4f57-b8c6-655e14164259",
   "metadata": {},
   "source": [
    "In this lesson, we used our knowledge of requests, beautiful soup and objects to build an indeed scraper.  The pattern that we used is called the adapter pattern.  With that pattern, we used a *client* to interact directly with the web site, and then passed the retrieved information to the adapter which extracted the related information and created a position instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
